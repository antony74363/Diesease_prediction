{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf3c3c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# General Import\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Building Model\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "\n",
    "# Training Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Data Processing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing.image import load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130309f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "SAVE = False\n",
    "SEED = 111\n",
    "\n",
    "# Setting seed for consistent results\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Data Visualization updates\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Data Classifications\n",
    "CLASS_TYPES = ['pituitary', 'notumor', 'meningioma', 'glioma']\n",
    "N_TYPES = len(CLASS_TYPES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72090224",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up file paths for training and testing\n",
    "DS_PATH = \"enter dataset path here\"\n",
    "train_dir = DS_PATH + r'/Training/'\n",
    "test_dir = DS_PATH + r'/Testing/'\n",
    "\n",
    "# Getting data using above function\n",
    "train_paths, train_labels = get_data_labels(train_dir)\n",
    "test_paths, test_labels = get_data_labels(test_dir)\n",
    "\n",
    "# Printing traing and testing sample sizes\n",
    "print('Training')\n",
    "print(f'Number of Paths: {len(train_paths)}')\n",
    "print(f'Number of Labels: {len(train_labels)}')\n",
    "print('\\nTesting')\n",
    "print(f'Number of Paths: {len(test_paths)}')\n",
    "print(f'Number of Labels: {len(test_labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f5e48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# getting image to test output\n",
    "im = load_img(train_paths[3], target_size=(150, 150))\n",
    "im = img_to_array(im)\n",
    "\n",
    "# Reshape it to (1, 150, 150, 3)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "print(f'x reshaped: {im.shape}')\n",
    "\n",
    "# normilzation tensor\n",
    "im /= np.max(im) # ~ np.max(img_tensor)\n",
    "\n",
    "# Convert the array back to the image format\n",
    "im = array_to_img(im[0])\n",
    "display(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2ec05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# getting image to test output\n",
    "im = load_img(train_paths[3], target_size=(150, 150))\n",
    "im = img_to_array(im)\n",
    "\n",
    "# Reshape it to (1, 150, 150, 3)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "print(f'x reshaped: {im.shape}')\n",
    "\n",
    "# normilzation tensor\n",
    "im /= np.max(im) # ~ np.max(img_tensor)\n",
    "\n",
    "# Convert the array back to the image format\n",
    "im = array_to_img(im[0])\n",
    "display(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca42cb4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " [10]:\n",
    "\n",
    "# Image size\n",
    "image_size = (150, 150)\n",
    "\n",
    "# Training batch size\n",
    "batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe6c02",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=10,\n",
    "                                   brightness_range=(0.85, 1.15),\n",
    "                                   width_shift_range=0.002,\n",
    "                                   height_shift_range=0.002,\n",
    "                                   shear_range=12.5,\n",
    "                                   zoom_range=0,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=False,\n",
    "                                   fill_mode=\"nearest\")\n",
    "\n",
    "\n",
    "# applying the generator to training data with constant seed\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=image_size,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    seed=SEED)\n",
    "\n",
    "# No augmentation of the test data, just rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# applying the generator to testing data with constant seed\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=image_size,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode=\"categorical\",\n",
    "                                                  shuffle=False,\n",
    "                                                  seed=SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411345c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Accessing class indices for training data generator\n",
    "class_indices_train = train_generator.class_indices\n",
    "class_indices_train_list = list(train_generator.class_indices.keys())\n",
    "\n",
    "\n",
    "# Displaying categorical types\n",
    "print(\"Categorical types for the training data:\")\n",
    "print(class_indices_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1be22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "show_ImageDataGenerator(train_datagen, num_samples=5, figsize=(12.5, 8), save=SAVE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639514f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Image shape: height, width, RBG\n",
    "image_shape = (image_size[0], image_size[1], 3)\n",
    "\n",
    "# Training epochs\n",
    "epochs = 40\n",
    "\n",
    "# Steps per epoch\n",
    "steps_per_epoch = train_generator.samples // batch_size\n",
    "\n",
    "# Validation steps\n",
    "validation_steps = test_generator.samples // batch_size\n",
    "\n",
    "print(f'Image shape: {image_shape}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Steps Per Epoch: {steps_per_epoch}')\n",
    "print(f'Validation steps: {validation_steps}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ee6cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model_1 = models.Sequential()\n",
    "\n",
    "# Convolutional layer 1\n",
    "model_1.add(Conv2D(32, (4, 4), activation=\"relu\", input_shape=image_shape))\n",
    "model_1.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "# Convolutional layer 2\n",
    "model_1.add(Conv2D(64, (4, 4), activation=\"relu\"))\n",
    "model_1.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "# Convolutional layer 3\n",
    "model_1.add(Conv2D(128, (4, 4), activation=\"relu\"))\n",
    "model_1.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "# Convolutional layer 4\n",
    "model_1.add(Conv2D(128, (4, 4), activation=\"relu\"))\n",
    "model_1.add(Flatten())\n",
    "\n",
    "# Full connect layers\n",
    "model_1.add(Dense(512, activation=\"relu\"))\n",
    "model_1.add(Dropout(0.5, seed=SEED))\n",
    "model_1.add(Dense(N_TYPES, activation=\"softmax\"))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be9018",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False, start_from_epoch=0)\n",
    "ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067785b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = models.Sequential([\n",
    "    \n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, (4, 4), activation=\"relu\", input_shape=image_shape),\n",
    "    MaxPooling2D(pool_size=(3, 3)),\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (4, 4), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(3, 3)),\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    Conv2D(128, (4, 4), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(3, 3)),\n",
    "\n",
    "    # Convolutional layer 4\n",
    "    Conv2D(128, (4, 4), activation=\"relu\"),\n",
    "    Flatten(),\n",
    "\n",
    "    # Full connect layers\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dropout(0.5, seed=SEED),\n",
    "    Dense(N_TYPES, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "optimizer = legacy.Adam(learning_rate=0.001, beta_1=0.869, beta_2=0.995)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca742a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from visualkeras import layered_view\n",
    "\n",
    "# Visualize the model\n",
    "layered_view(model, legend=True, max_xy=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba53afb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_visual = models.Model(inputs=model.input, outputs=model.output)\n",
    "\n",
    "# Save model architecture to a file\n",
    "plot_model(model_visual, show_dtype=True, to_file='model_architecture.png', show_shapes=True)\n",
    "\n",
    "# Display model architecture in the notebook\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(retina=True, filename='model_architecture.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a430e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stop training if loss doesn't keep decreasing.\n",
    "model_es = EarlyStopping(monitor='loss', min_delta=1e-9, patience=8, verbose=True)\n",
    "model_rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, verbose=True)\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[model_es, model_rlr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29576ffe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples//batch_size)\n",
    "print(f\"Test Loss: {loss:0.5f}\")\n",
    "print(f\"Test Accuracy: {accuracy:0.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364ab74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(ncols=2, figsize=(15, 6))\n",
    "\n",
    "# Plot the training and validation accuracy over epochs\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('Model 2 Accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['Train', 'Validation'])\n",
    "ax[0].grid(alpha=0.2)\n",
    "\n",
    "# Plot the training and validation loss over epochs\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('Model 2 Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['Train', 'Validation'])\n",
    "ax[1].grid(alpha=0.2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83635a65",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plotting confusion matrix\n",
    "confusion_matrix = CM(CNN_model=model, test_generator=test_generator, categories=class_indices_train_list)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.xticks(ticks=np.arange(N_TYPES) + 0.5,\n",
    "           labels=[name.title() for name in class_indices_train_list], ha='center')\n",
    "plt.yticks(ticks=np.arange(N_TYPES) + 0.5, \n",
    "           labels=[name.title() for name in class_indices_train_list], va='center')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373312d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Showing metrics\n",
    "calculate_metrics(confusion_matrix, categories=class_indices_train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0068f46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[26]:\n",
    "\n",
    "# Using functions in 6.1 for showing results\n",
    "plot_sample_predictions(model=model, \n",
    "                        test_generator=test_generator, \n",
    "                        categories=class_indices_train_list,\n",
    "                        test_dir=test_dir, \n",
    "                        num_samples=9,\n",
    "                        figsize=(13, 12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07809918",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the next batch from the test generator\n",
    "batch_images, batch_labels = next(test_generator)\n",
    "\n",
    "# Extract the first image from the batch\n",
    "image, label = batch_images[0], batch_labels[0]\n",
    "image_tensor = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Get the class indices from the test generator\n",
    "class_indices = test_generator.class_indices\n",
    "\n",
    "# Convert the one-hot encoded label to the class name\n",
    "label_name = [k for k, v in class_indices.items() if np.argmax(label) == v][0]\n",
    "\n",
    "# Display the class name\n",
    "print(f\"Class name of the first image: {label_name}\")\n",
    "print(f'Shape {image_tensor.shape}')\n",
    "array_to_img(image_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212ae8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_channel_activation_maps(model=model, image=image_tensor, N=5, save=SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90b13c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Using function above \n",
    "visualize_misclassified_images(model, test_generator, test_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d24def",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"cnn_brain_tumour.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
